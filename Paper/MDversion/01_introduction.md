---
source_chat_name:
  - JPEP introduction writing [accidentally deleted]
  - JPEP post-completion introduction rewriting
source_chat_id:
  - "[not available]"
  - ae493f0b-cc8a-43b0-b32f-0fc597b297a2
---

# 1. Introduction

Luciano Floridi recently introduced the concept of "distant writing" to characterize literary production assisted by large language models (Floridi 2025a). Authors using AI systems function as "meta-authors" who design narratives while LLMs perform the actual writing—what Floridi terms *wrAIting*, a practice distinct from traditional authorship. Floridi's focus is literary production; he does not explore whether this framework applies to philosophical scholarship, where writing and thinking are not cleanly separable. What remains absent is a phenomenological account of AI-assisted scholarly work that addresses how philosophical arguments emerge through iterative processes where prompting, reading, and revising constitute rather than merely express intellectual development.

The philosophical community has engaged these developments primarily through two distinct debates, neither of which addresses the structural problems facing scholars who work transparently with AI systems. The first debate concerns pedagogy and has generated considerable anxiety. In Daily Nous discussions, Troy Jollimore describes teaching in an environment where "faith has been obliterated" (Jollimore 2025), while Anastasia Berg characterizes AI tools as "rendering students subcognitive" (Berg & Robbins 2024), and Regina Rini anticipates "a miserable war of attrition" between instructors and students using ChatGPT (Rini 2025). This pedagogical crisis, whatever its ultimate significance, lies outside this article's scope. Teaching and learning raise distinct questions about cognitive development, assessment validity, and educational ethics that do not directly bear on the production of scholarship by established researchers.

The second debate addresses research potential. Christopher Ontiveros and Gordon Clay argue that AI could provide "a suite of tools" to revolutionize philosophical practice if properly developed (Ontiveros & Clay 2021). This optimistic assessment, however, concerns specialized AI systems designed for particular research tasks—automated argument mapping, large-scale literature review, logical consistency checking—rather than the general-purpose language models scholars currently use for writing assistance. More moderate voices observe that delegation of writing tasks already occurs routinely in scientific collaboration, suggesting resistance to AI assistance may reflect *AI exceptionalism* rather than principled concern about distributed authorship. Yet this reasonable observation does not address the specific challenges facing scholars who use AI systems extensively and disclose this use openly.

Current journal policies reflect near-unanimous consensus on two points. First, AI systems cannot be listed as authors. Following COPE criteria, major publishers including Elsevier, ACM, and Science journals prohibit AI authorship on grounds that LLMs lack accountability and cannot approve final manuscript versions (COPE Council 2024; Elsevier 2023; ACM 2025; Science 2023). A 2023 study by Lund and Naheem found that over half of major journals have implemented AI policies, typically requiring disclosure in Methods or Acknowledgments sections (Lund & Naheem 2023). Second, disclosure of AI assistance is mandatory when such tools contribute substantively to manuscript preparation. These policies establish clear boundaries: AI systems are tools, not collaborators; their use must be disclosed but minimized; the human author retains full responsibility.

Yet this policy consensus operates without philosophical clarity about what values transparency is meant to serve. Is disclosure intended to enable readers to assess epistemic reliability? To maintain research integrity through fraud prevention? To facilitate methodological learning across the scholarly community? To allow verification that human intellectual work occurred rather than mere AI text generation? Current policies do not specify, and this silence creates practical problems. Without understanding what transparency is *for*, we cannot determine what constitutes adequate disclosure, assess whether existing disclosure practices serve their intended purposes, or develop meaningful standards to distinguish substantial AI contributions from minimal ones. The agreement on mandatory disclosure masks a deeper disagreement—or perhaps a collective failure to articulate—about the epistemic and ethical principles that such disclosure should uphold.

This ambiguity creates a paradox. Policies demand transparency about AI involvement while the broader academic culture penalizes such transparency. Disclosure satisfies integrity requirements but carries professional costs. Reviewers may dismiss disclosed AI-assisted work as "AI slop" regardless of argument quality. Editors risk alienating their reviewer pools by appearing to take such submissions seriously. The professional stigma attached to substantial AI assistance creates incentives to minimize reported involvement. More perversely, these incentives operate most powerfully for scholars' most important work. For a potentially career-defining article, authors face maximum motivation to underreport AI contributions since the disclosure becomes permanently attached to a high-value professional asset. Put yourself in the shows of a future historian of philosophy: the greatest 21th century philosophers (or in the first century PC, *post-Chat*) who will be identified as humans will have their reputation tainted by a lingering doubt about the role, if any, that LLM chatbots may have played in creating their masterworks.

This article identifies four structural gaps in current debates about AI-assisted scholarship. First, no venue exists for fully disclosed substantial AI assistance. All existing policies assume AI involvement will be minimal—a tool used occasionally, not a sustained collaborative practice. Second, discussions remain entirely defensive, focused on preventing abuse rather than articulating any positive case for AI-assisted scholarship. Third, the framework remains binary: human author versus machine tool, with no conceptual resources for understanding genuinely distributed intellectual production. Fourth, while policies require disclosure of AI involvement, no mechanism exists for validating such disclosures. Authors self-report their dependence on AI systems, but journals provide no means to verify these reports or assess whether they accurately characterize the work's production.

These gaps reveal a deeper methodological absence: while research evaluates the outputs of AI-assisted writing and debates the ethics of AI tool use, phenomenological accounts of the lived experience of AI-assisted scholarly composition remain scarce. This absence is particularly acute for philosophical work, where the dialectical refinement of arguments through conversation represents not mere writing assistance but a transformation of the intellectual process itself. This absence destroys all reliable evidence of *system 0* cognition (Chiriatti et al, 2024) within professional philosophy, the “artificial, non-biological underlying layer of distributed intelligence that interacts with and augments both intuitive and analytical thinking processes,” (p. 1829) and therefore masks the biases such cognition may bring.

This article explores—and itself exemplifies—infrastructure and processes that might enable productive examination of AI-assisted scholarship. Rather than arguing definitively that AI-assisted scholarship should or should not be accepted, it designs a new refereeing procedure—to be implemented either by a distinct journal or a dedicated track within an existing journal—that could address the transparency paradox by creating space where substantial AI assistance can be disclosed fully without professional penalty. Moreover it argues that, if such approaches prove viable, they might serve three functions. First, they could provide space where transparency is valued rather than penalized, addressing the incentive structure that currently rewards dishonesty. Second, they might enable articulation of positive cases for AI-assisted philosophical work, moving beyond purely defensive framings. Third, they could implement review mechanisms that validate rather than merely police disclosure, testing whether authors' prompts contain sufficient intellectual content to generate the arguments they claim as their own.

This article asks readers to evaluate a transparency framework through an example that displays the framework's value. The argument for validated disclosure of AI assistance is developed through extensively AI-assisted writing; the artifact ontology for phenomenological examination of assisted scholarship emerges directly from within that practice, during writing, and this process itself is documented through those very artifacts. This circularity serves an epistemic function: the article demonstrates what full transparency might look like while simultaneously testing it, in an ecologically significant experiment (creative writing, with ongoing revisions of a rough initial idea). Readers can evaluate both the substantive arguments about publishing infrastructure and the meta-level question of whether this mode of intellectual production and documentation merits scholarly consideration.

This article begins to build this infrastructure with a vision + proof-of-concept. I offer a worked blueprint—an integrated venue design and a dual-review architecture with a trajectory-matching reproduction test—together with the operational materials already provided in the Appendix (Documentation Structure and Reproduction Procedure), including the SP-1–SP-5 package and a concise reproduction guide/protocol. This article, however, specifically aims to live as a philosophical article among other articles in a traditional journal. However, its detailed Artifact (as long as the article itself) and the (even longer) documentation in the Supplementary Material is proposed as a template for immediate piloting by willing experimenters. A proper assessment would simply require a small-scale pilot using predeclared criteria, while this present article specifies what artifacts seem sufficient for initial attempts to test the idea. This is not yet intended as a scientific, empirical experiment.

The argument proceeds in three movements. Section 2 analyzes the systemic barriers to disclosure, demonstrating how current incentive structures create a gradient where underreporting increases with perceived significance. Section 3 examines why philosophy might engage substantively with AI-assisted scholarship despite these barriers. Section 4 reframes the apparent dilemma between prestige and transparency, arguing that strategic positioning outside traditional metrics could enable long-term transformation through identifiable feedback mechanisms. Sections 5 through 7 specify operational infrastructure: Section 5 details the discontinuity from prestige systems that creates design space for ecological validity, good faith orientation, and costly signaling; Section 6 articulates what mandatory transparency means in practice, including the concrete disclosure requirements this article implements; Section 7 describes the dual-reviewer architecture with trajectory-matching reproduction test that validates disclosed methodologies while enabling methodological learning. The Appendix provides comprehensive documentation of this article's production process, organized as five supplementary packages (SP-1 through SP-5) that enable reproduction testing. These Artifacts—comparable in length to the article itself—document the complete developmental trajectory through conversation logs, modification records, and prompt evolution, serving simultaneously as transparency model, research material, and methodological contribution for scholars interested in AI-assisted philosophical work. Both in a normal submission and in the special procedure I describe, these should be made available as supplementary material for both reviewers and the general readership of a journal.
