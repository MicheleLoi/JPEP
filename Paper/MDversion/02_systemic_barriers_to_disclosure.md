---
source_chat_name:
  - "JPEP section 2 writing"
  - "JPEP section 3 writing"
  - "JPEP section 4 writing"
  - "JPEP post-editorial review of Section 4 [2]"
  - "JPEP consolidated 2 writing"
source_chat_id:
  - "4177422b-27c3-44d4-a52e-f065de4e72ab"
  - "6e92907a-03f7-413f-b99f-2983f8f44b22"
  - "17c34bca-3050-4825-908a-599adfa9f8c0"
  - "277c8d57-07af-45b2-9a83-a36a6d9b4d0d" 
  - "ffea5b8a-9c81-46c9-bb3c-8138d45c8eec"
---


# 2. Systemic Barriers to Disclosure

Current publishing policies require disclosure of AI assistance while simultaneously creating professional environments where such disclosure carries reputational costs. This section analyzes how the resulting incentive structure interacts with institutional constraints to produce systematic underreporting of AI involvement, particularly for work scholars regard as most significant. The analysis reveals not merely individual decisions about disclosure, but a systemic problem resistant to obvious solutions.

## 2.1 The Incentive Gradient

Disclosure decisions occur under asymmetric conditions. Disclosure is permanent—methodological characterizations become part of the scholarly record, traveling with work through citations and career evaluations. Significance is uncertain—authors cannot predict at submission which articles will prove influential. Professional costs are front-loaded—stigma associated with disclosed AI assistance operates immediately, affecting initial reception regardless of eventual impact.

These asymmetries interact to create a gradient of pressure toward underreporting. For work regarded as minor or routine, honest disclosure carries relatively low cost: methodological details function as scholarly housekeeping. For potentially significant work, disclosure becomes fraught—authors face the dilemma of reporting honestly and risking dismissal as methodologically suspect, or minimizing AI involvement to preserve perceived legitimacy. For work regarded as career-defining, the incentive to underreport reaches maximum strength: disclosure becomes permanently attached to the scholar's primary career asset. The gradient operates continuously, but the directional pressure is clear—as perceived significance increases, reported AI involvement likely decreases.

The underreporting need not require conscious dishonesty. Several plausible mechanisms can be identified that could operate even among scholars committed to general honesty, each exploiting genuine ambiguities in how AI-assisted processes might be characterized.

The first mechanism involves *definitional flexibility*. Terms like "substantial AI assistance" and "minimal editorial support" lack precise boundaries, particularly given the varied ways scholars might employ AI tools. An author who engaged in extensive AI-assisted exploration during early argument development but then substantially revised all prose in later drafts would face genuine uncertainty about proper characterization. When professional stakes are high, this uncertainty could resolve in favor of lower reported involvement. The author might focus on the final revision phase where human control was indeed substantial, characterizing earlier AI-assisted exploration as preliminary rather than constitutive of the final argument.

This connects to a second mechanism: *temporal discounting of early-stage AI involvement*. Scholars might naturally privilege later stages of work when forming narrative accounts of their process. If initial argument exploration involved significant AI dialogue but final drafting required extensive human revision, authors could retrospectively frame the AI involvement as scaffolding that was replaced rather than as integral to the work's development. The temporal structure of the process—AI-heavy early stages, human-heavy later stages—would allow authors to construct accounts that minimize AI's role without explicit falsehood.

A third mechanism could operate through *comparative framing*. Authors might compare their process not to traditional unassisted writing but to hypothetical greater AI dependence: "I didn't merely accept ChatGPT's suggestions; I critically evaluated and substantially revised everything." This comparison would not be false—the author did indeed revise and evaluate. But the reference point shifts from "traditional unassisted scholarship" to "passive acceptance of AI output," making the actual process appear more human-centered than it might be relative to traditional standards. Crucially, this framing could occur without conscious deception. Authors could genuinely believe they are reporting honestly when they emphasize the critical evaluation they performed, even as this emphasis obscures the shift from traditional scholarly norms. The mechanism would operate through sincere belief in accounts that happen to serve professional interests.

Finally, *strategic vagueness* would allow authors to satisfy disclosure requirements while preserving interpretive flexibility. Phrases like "AI tools were consulted during drafting" or "language models assisted with argument development" provide formal compliance without specifying the degree of involvement. Journal policies requiring disclosure rarely specify granularity or detail level, potentially creating space for minimally informative but formally adequate statements. An author could truthfully report AI involvement while leaving readers unable to assess whether this involvement was peripheral or central to the work's development.

If these mechanisms operate as described, they combine to allow systematic minimization of reported AI involvement particularly when work appears professionally significant. The result would not be deliberate fraud but rather predictable response to incentive structures that penalize transparency while demanding formal disclosure.

The incentive structure creates a *transparency paradox*. Where transparency matters most, we get least. The work most likely to shape scholarly discourse—articles that will be widely cited, taught, and built upon—faces the strongest pressure to underreport AI involvement. Conversely, forgettable minor contributions face lower costs for honest disclosure but matter less to the scholarly record. The result inverts the ideal relationship between significance and transparency.

Moreover, disclosure requirements produce *minimal disclosure*. Policies requiring authors to report AI assistance create formal compliance without substantive transparency. Authors report something—enough to satisfy editorial requirements—while minimizing the reported degree of involvement. The result resembles tax code compliance: formally adequate while structured to minimize apparent obligation, potentially creating space for compliance that satisfies letter but not spirit.

## 2.2 Institutional Design Constraints

If the incentive problems identified in 2.1 are structural rather than individual, perhaps creating a new venue for AI-assisted scholarship could solve them. This subsection examines why approaches maintaining continuity with traditional prestige structures inherit these incentive problems, while approaches rejecting such continuity risk marginalization—and why both problems connect to systematic resistance within review processes themselves.

Consider a venue designed to accommodate AI-assisted scholarship while positioning itself in the traditional way. Such a venue might require disclosure of AI involvement while maintaining traditional peer review standards, aspire to eventual indexing in established databases, and frame itself as experimental but serious—a space where scholars can test these methods while building work that might ultimately contribute to broader scholarly conversation.

Given 2.1's analysis, the structural problem becomes apparent: if scholars understand that work published in this venue could be cited in prestigious traditional journals, could count (even partially) toward tenure evaluations, or could eventually contribute to standard scholarly records, then they face the same incentive structure. The asymmetries—permanent disclosure, uncertain significance, front-loaded professional costs—remain operative. When stakes are potentially high, the mechanisms of underreporting identified above remain available and professionally rational.

Plausibly, the incentive problems arise from connection to prestige structures. Perhaps, then, the solution requires explicit discontinuity. Consider a venue that openly positions itself as separate from traditional scholarly infrastructure—a space that does not seek indexing in established databases, explicitly does not count for tenure or hiring decisions, and frames itself as serving different values than those dominant in traditional publishing.

Such separation could address the incentive problem. If scholars understand that work in this venue will not contribute to traditional career advancement, the "just in case" reasoning loses its purchase. There is no potential future in which disclosure decisions made in this venue become consequential for professional advancement in the traditional system. An author might therefore feel substantially freer to disclose the actual extent of AI involvement without strategic minimization.

But this solution faces its own structural problem: if the venue is genuinely discontinuous from traditional prestige structures, it risks irrelevance. Scholars face significant opportunity costs when allocating research time. Every article submitted to a venue that does not count for career advancement represents time not spent on work that does count. For scholars navigating competitive markets or tenure processes, these costs may be prohibitive. The venue risks becoming either a space where established scholars pursue projects unconstrained by career considerations, or a refuge for work that could not succeed in traditional venues. In either case, the venue's separation signals "not serious scholarship." Moreover, marginalization undermines the knowledge-production goals motivating such a venue. Understanding how AI-assisted scholarship actually works requires participation from scholars actively engaged with cutting-edge work, not recreational scholarship from established figures or work that couldn't succeed elsewhere.

These institutional constraints interact with evaluation practices to compound the problem. Even if scholars were willing to disclose substantial AI involvement honestly, they may encounter systematic resistance within traditional review processes. Two distinct problems emerge. First, some reviewers may react negatively to being asked to review what they regard as methodologically compromised work. This resistance need not depend on the argument's quality. The disclosure itself may trigger dismissal before substantive engagement occurs. A reviewer who believes AI-assisted work inherently lacks scholarly rigor will not evaluate such work by the same standards applied to traditionally produced scholarship, regardless of what the text demonstrates.

Second, even editors who might evaluate AI-assisted work fairly face a coordination problem. Assigning such work to reviewers carries professional risk. If reviewers respond negatively—whether through declining to review, providing dismissive assessments, or expressing frustration at being sent such material—editors bear the cost of appearing to take methodologically controversial work seriously. The professional incentive structure favors risk aversion.

These evaluation barriers compound the incentive problems. A scholar might be willing to disclose substantial AI involvement honestly, having decided the work's significance justifies the professional cost. But if venues cannot provide fair evaluation regardless of disclosure quality, then transparency becomes professionally untenable even for those committed to it.
